import requests
from bs4 import BeautifulSoup, Tag
from typing import Optional, Dict, List
import json
import re  # Dodaj import modu≈Çu re dla wyra≈ºe≈Ñ regularnych
from datetime import datetime
from urllib.parse import quote

BASE_URL = "https://www.dekudeals.com"


def parse_release_dates(raw_text: str) -> Dict[str, str]:
    """
    Parsuje surowy tekst dat wydania na strukturƒô platform -> data.

    Input przyk≈Çad: ":PS4, SwitchJanuary 25, 2018Xbox OneJanuary 26, 2018"
    Output: {"PS4": "January 25, 2018", "Switch": "January 25, 2018", "Xbox One": "January 26, 2018"}
    """
    if not raw_text or raw_text.strip() == ":":
        return {"unknown": "Unknown release date"}

    # Usu≈Ñ dwukropek z poczƒÖtku
    clean_text = raw_text.lstrip(":").strip()

    # Znane platformy
    platforms = [
        "PlayStation 5",
        "PS5",
        "PlayStation 4",
        "PS4",
        "Xbox Series X/S",
        "Xbox Series",
        "Xbox One",
        "Switch",
        "Nintendo Switch",
        "PC",
        "Steam",
    ]

    # Regex dla dat w formacie "Month Day, Year"
    date_pattern = r"([A-Z][a-z]+\s+\d{1,2},\s+\d{4})"

    # Znajd≈∫ wszystkie daty
    dates = re.findall(date_pattern, clean_text)

    if not dates:
        return {"all_platforms": clean_text}

    result = {}
    current_text = clean_text

    # Dla ka≈ºdej znalezionej daty, znajd≈∫ platformy przed niƒÖ
    for date in dates:
        # Znajd≈∫ pozycjƒô daty w tek≈õcie
        date_pos = current_text.find(date)
        if date_pos == -1:
            continue

        # Tekst przed datƒÖ zawiera platformy
        before_date = current_text[:date_pos]

        # Znajd≈∫ platformy w tek≈õcie przed datƒÖ
        found_platforms = []
        for platform in platforms:
            if platform in before_date:
                found_platforms.append(platform)
                # Usu≈Ñ znalezionƒÖ platformƒô z tekstu, ≈ºeby nie duplikowaƒá
                before_date = before_date.replace(platform, "", 1)

        # Je≈õli znaleziono platformy, przypisz im datƒô
        if found_platforms:
            for platform in found_platforms:
                result[platform] = date
        else:
            # Je≈õli nie znaleziono konkretnych platform, u≈ºyj generycznego klucza
            if "unknown_platforms" not in result:
                result["unknown_platforms"] = []
            result["unknown_platforms"].append(date)

        # Usu≈Ñ przetworzonƒÖ czƒô≈õƒá z tekstu
        current_text = current_text[date_pos + len(date) :]

    return result if result else {"all_platforms": clean_text}


def search_deku_deals(query: str) -> Optional[str]:
    """
    Wyszukuje grƒô na DekuDeals.com i zwraca URL do jej strony produktu.
    Zwraca None, je≈õli gra nie zosta≈Ça znaleziona w pierwszych wynikach.
    """
    search_url = f"{BASE_URL}/search?q={quote(query)}"
    print(f"Szukam gry '{query}' na: {search_url}")

    try:
        response = requests.get(search_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # Poprawiony selektor na podstawie Twojego odkrycia
        first_result_link = soup.find("a", class_="main-link")

        if (
            first_result_link
            and isinstance(first_result_link, Tag)
            and first_result_link.get("href")
        ):
            game_path = first_result_link.get("href")
            full_game_url = f"{BASE_URL}{game_path}"
            print(f"Znaleziono potencjalny URL dla '{query}': {full_game_url}")
            return full_game_url
        else:
            print(f"Nie znaleziono bezpo≈õredniego linku do gry dla '{query}'.")
            return None

    except requests.exceptions.RequestException as e:
        print(f"B≈ÇƒÖd sieciowy podczas wyszukiwania gry '{query}': {e}")
        return None
    except Exception as e:
        print(f"Nieoczekiwany b≈ÇƒÖd podczas parsowania wynik√≥w wyszukiwania: {e}")
        return None


def scrape_game_details(game_url: str) -> Optional[Dict]:
    """
    Scrapuje szczeg√≥≈Çowe dane o grze z jej strony DekuDeals.
    Zwraca s≈Çownik z danymi lub None w przypadku b≈Çƒôdu/braku danych.
    """
    print(f"Scrapujƒô szczeg√≥≈Çy z URL: {game_url}")

    try:
        response = requests.get(game_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        game_details = {}

        # --- Tytu≈Ç Gry ---
        # Widaƒá, ≈ºe tytu≈Ç jest w <span class='display-5 item-title'> wewnƒÖtrz <h2>
        title_tag = soup.find("span", class_="item-title")
        if title_tag:
            game_details["title"] = title_tag.get_text(strip=True)
        else:
            game_details["title"] = "Nieznany tytu≈Ç"

        # --- Sekcja 'Details' (list-group) ---
        details_list = soup.find("ul", class_="details")
        if details_list:
            # Iteruj przez wszystkie elementy listy w sekcji 'Details'
            for li in details_list.find_all("li", class_="list-group-item"):
                strong_tag = li.find("strong")
                if not strong_tag:
                    continue  # Pomijamy elementy bez strong (np. te zagnie≈ºd≈ºone ul dla player modes)

                label = strong_tag.get_text(strip=True).replace(":", "")  # Np. 'MSRP'

                # U≈ºyj find_next_sibling('dd') dla pewno≈õci, lub po prostu .find() dla a
                # Sprawd≈∫, co nastƒôpuje po strong i jak wyciƒÖgnƒÖƒá tekst

                # --- MSRP ---
                if "MSRP" in label:
                    # Cena jest bezpo≈õrednio po strong, lub w innerHTML
                    msrp_text = li.get_text(strip=True).replace(label, "").strip()
                    game_details["MSRP"] = msrp_text

                # --- Release date ---
                elif "Release date" in label:
                    release_date_text = (
                        li.get_text(strip=True).replace(label, "").strip()
                    )
                    # Parsuj surowy tekst na strukturƒô u≈ºytecznƒÖ dla AI
                    parsed_release_dates = parse_release_dates(release_date_text)
                    game_details["release_date"] = (
                        release_date_text  # Zachowaj surowy dla debug
                    )
                    game_details["release_dates_parsed"] = (
                        parsed_release_dates  # Dodaj sparsowane dane
                    )

                # --- Genre ---
                elif "Genre" in label:
                    genres = [
                        a.get_text(strip=True)
                        for a in li.find_all(
                            "a", href=re.compile(r"/games\?filter\[genre\]=")
                        )
                    ]
                    game_details["genres"] = genres if genres else ["Nieznany"]

                # --- Developer ---
                elif "Developer" in label:
                    dev_tag = li.find(
                        "a", href=re.compile(r"/games\?filter\[developer\]=")
                    )
                    if dev_tag:
                        game_details["developer"] = dev_tag.get_text(strip=True)
                    else:
                        game_details["developer"] = "Nieznany"

                # --- Publisher ---
                elif "Publisher" in label:
                    pub_tag = li.find(
                        "a", href=re.compile(r"/games\?filter\[publisher\]=")
                    )
                    if pub_tag:
                        game_details["publisher"] = pub_tag.get_text(strip=True)
                    else:
                        game_details["publisher"] = "Nieznany"

                # --- Metacritic ---
                elif "Metacritic" in label:
                    metacritic_link = li.find("a", class_="metacritic")
                    if metacritic_link:
                        # Pierwszy span to wynik Metacritic, drugi to User Score (opcjonalnie)
                        scores = metacritic_link.find_all("span")
                        if scores:
                            game_details["metacritic_score"] = scores[0].get_text(
                                strip=True
                            )
                        if len(scores) > 1:
                            game_details["metacritic_user_score"] = scores[1].get_text(
                                strip=True
                            )
                    else:
                        game_details["metacritic_score"] = "Brak oceny"
                        game_details["metacritic_user_score"] = "Brak oceny"

                # --- OpenCritic ---
                elif "OpenCritic" in label:
                    opencritic_link = li.find("a", class_="opencritic")
                    if opencritic_link:
                        # Wynik jest bezpo≈õrednio po div'ie z klasƒÖ 'opencritic-tier'
                        # Lub jest ostatnim tekstem w linku a
                        score_text = "".join(
                            opencritic_link.find_all(string=True, recursive=False)
                        ).strip()
                        game_details["opencritic_score"] = score_text
                    else:
                        game_details["opencritic_score"] = "Brak oceny"

                # --- Platformy ---
                # Jak zauwa≈ºy≈Çe≈õ, "Platforms" to ostatni element <li> w 'details' list
                # Sprawd≈∫, czy `li` zawiera tekst "Platforms", a nastƒôpnie pobierz jego tekst
                if "Platforms" in label:  # `Platforms:`
                    # Tekst platformy jest bezpo≈õrednio w li, po strong
                    platform_text = li.get_text(strip=True).replace(label, "").strip()
                    game_details["platform"] = platform_text

        else:
            print("Nie znaleziono sekcji 'Details'.")

        # --- Aktualne Ceny (z tabeli) ---
        # Zidentyfikowana tabela: <table class='table table-align-middle item-price-table'>
        price_table = soup.find("table", class_="item-price-table")
        if price_table:
            # Pierwszy wiersz (tr) z cenƒÖ jest zazwyczaj aktualnƒÖ cenƒÖ eShop (digital)
            first_price_row = price_table.find("tr")
            if first_price_row:
                price_button_tag = first_price_row.find(
                    "div", class_="btn-primary"
                )  # Przycisk z cenƒÖ
                if price_button_tag:
                    game_details["current_eshop_price"] = price_button_tag.get_text(
                        strip=True
                    )
                else:
                    game_details["current_eshop_price"] = "N/A"
            else:
                game_details["current_eshop_price"] = "N/A"
        else:
            print("Nie znaleziono tabeli cen.")
            game_details["current_eshop_price"] = "N/A"

        # --- Najni≈ºsza Cena w Historii ---
        # Znajduje siƒô w sekcji 'Price history'
        price_history_section = soup.find("div", id="price-history")
        if price_history_section:
            # Szukaj tekstu 'All time low' i obok niego ceny
            all_time_low_row = price_history_section.find(
                "strong", string="All time low"
            )
            if all_time_low_row:
                # Cena jest w nastƒôpnym <td> po <tr> zawierajƒÖcym 'All time low'
                # lub w td z klasƒÖ 'text-right pl-3'
                lowest_price_td = (
                    all_time_low_row.find_parent("tr")
                    .find_next_sibling("tr")
                    .find("td", class_="text-right")
                )
                if lowest_price_td:
                    game_details["lowest_historical_price"] = lowest_price_td.get_text(
                        strip=True
                    )
                else:
                    game_details["lowest_historical_price"] = (
                        "Brak danych o najni≈ºszej cenie"
                    )
            else:
                game_details["lowest_historical_price"] = (
                    "Brak danych o najni≈ºszej cenie"
                )
        else:
            print("Nie znaleziono sekcji 'Price history'.")
            game_details["lowest_historical_price"] = "Brak danych o historii cen"

        print(f"Szczeg√≥≈Çy zebrane dla {game_details.get('title', 'gry')}:")

        return game_details

    except requests.exceptions.RequestException as e:
        print(f"B≈ÇƒÖd sieciowy podczas scrapowania szczeg√≥≈Ç√≥w z '{game_url}': {e}")
        return None
    except Exception as e:
        print(f"Nieoczekiwany b≈ÇƒÖd podczas parsowania szczeg√≥≈Ç√≥w z '{game_url}': {e}")
        return None


def scrape_dekudeals_collection(collection_url: str) -> Dict[str, any]:
    """
    Scrapuje kolekcjƒô gier z DekuDeals i zwraca listƒô nazw gier.

    Args:
        collection_url (str): URL do kolekcji DekuDeals (np. https://www.dekudeals.com/collection/xxx)

    Returns:
        Dict zawierajƒÖcy:
        - success (bool): Czy operacja siƒô powiod≈Ça
        - games (List[str]): Lista nazw gier
        - game_count (int): Liczba znalezionych gier
        - error (str): B≈ÇƒÖd je≈õli wystƒÖpi≈Ç
    """
    try:
        print(f"Parsuje kolekcjƒô DekuDeals: {collection_url}")

        # Walidacja URL
        if not collection_url or "dekudeals.com/collection/" not in collection_url:
            return {
                "success": False,
                "error": "Nieprawid≈Çowy URL kolekcji DekuDeals",
                "games": [],
                "game_count": 0,
            }

        # Pobierz stronƒô kolekcji
        response = requests.get(collection_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # Lista do przechowywania nazw gier
        game_titles = []

        # Znajd≈∫ wszystkie gry w kolekcji
        # Na podstawie struktury strony, szukamy element√≥w kt√≥re zawierajƒÖ nazwy gier
        # Mo≈ºliwe selektory dla gier w kolekcji
        game_selectors = [
            "h3",  # G≈Ç√≥wne nag≈Ç√≥wki gier
            ".game-title",  # Ewentualne klasy z tytu≈Çami
            "[data-game-title]",  # Elementy z atrybutami tytu≈Ç√≥w
            "a[href*='/items/']",  # Linki do stron gier
        ]

        # Pr√≥bujemy r√≥≈ºne selektory
        for selector in game_selectors:
            elements = soup.select(selector)
            if elements:
                print(
                    f"Znaleziono {len(elements)} element√≥w u≈ºywajƒÖc selektora: {selector}"
                )

                for element in elements:
                    if isinstance(element, Tag):
                        # WyciƒÖgnij tekst z elementu
                        title_text = element.get_text(strip=True)

                        # Sprawd≈∫ czy to wyglƒÖda jak tytu≈Ç gry (nie jest pusty, nie zawiera tylko cyfr)
                        if (
                            title_text
                            and len(title_text) > 2
                            and not title_text.isdigit()
                        ):
                            # Czy≈õƒá tytu≈Ç z niepotrzebnych element√≥w
                            cleaned_title = clean_game_title(title_text)
                            if cleaned_title and cleaned_title not in game_titles:
                                game_titles.append(cleaned_title)

                # Je≈õli znale≈∫li≈õmy gry, przerywamy pƒôtlƒô
                if game_titles:
                    break

        # Je≈õli nie znale≈∫li≈õmy gier standardowymi metodami, spr√≥buj alternatywnych
        if not game_titles:
            print("Pr√≥bujƒô alternatywnych metod parsowania...")

            # Szukaj w ca≈Çym tek≈õcie strony tytu≈Ç√≥w gier
            # Mo≈ºemy szukaƒá wzorc√≥w tekstowych charakterystycznych dla gier
            all_text = soup.get_text()

            # Podziel tekst na linie i szukaj potencjalnych tytu≈Ç√≥w gier
            lines = all_text.split("\n")
            for line in lines:
                line = line.strip()
                # Sprawd≈∫ czy linia wyglƒÖda jak tytu≈Ç gry
                if (
                    line
                    and len(line) > 3
                    and len(line) < 100
                    and not line.isdigit()
                    and not line.startswith("$")
                    and not line.lower().startswith("rating")
                    and not line.lower().startswith("format")
                    and not line.lower().startswith("platform")
                ):

                    # Dodatkowe filtrowanie dla typowych tytu≈Ç√≥w gier
                    if any(
                        keyword in line.lower()
                        for keyword in [
                            "edition",
                            "collection",
                            "ultimate",
                            "deluxe",
                            "remastered",
                            ":",
                        ]
                    ):
                        cleaned_title = clean_game_title(line)
                        if cleaned_title and cleaned_title not in game_titles:
                            game_titles.append(cleaned_title)

        # Usu≈Ñ duplikaty zachowujƒÖc kolejno≈õƒá
        unique_titles = []
        seen = set()
        for title in game_titles:
            if title not in seen:
                unique_titles.append(title)
                seen.add(title)

        print(f"Znaleziono {len(unique_titles)} unikalnych gier w kolekcji")

        return {
            "success": True,
            "games": unique_titles,
            "game_count": len(unique_titles),
            "collection_url": collection_url,
        }

    except requests.exceptions.RequestException as e:
        error_msg = f"B≈ÇƒÖd sieciowy podczas pobierania kolekcji: {str(e)}"
        print(error_msg)
        return {"success": False, "error": error_msg, "games": [], "game_count": 0}
    except Exception as e:
        error_msg = f"B≈ÇƒÖd podczas parsowania kolekcji: {str(e)}"
        print(error_msg)
        return {"success": False, "error": error_msg, "games": [], "game_count": 0}


def clean_game_title(title: str) -> str:
    """
    Czy≈õci tytu≈Ç gry z niepotrzebnych element√≥w.

    Args:
        title (str): Surowy tytu≈Ç gry

    Returns:
        str: Oczyszczony tytu≈Ç gry
    """
    if not title:
        return ""

    # Usu≈Ñ bia≈Çe znaki z poczƒÖtku i ko≈Ñca
    cleaned = title.strip()

    # Usu≈Ñ typowe s≈Çowa/frazy kt√≥re nie sƒÖ czƒô≈õciƒÖ tytu≈Çu
    unwanted_patterns = [
        r"\s*Rating\s*",
        r"\s*Format\s*",
        r"\s*Platform\s*",
        r"\s*Digital\s*",
        r"\s*Switch\s*$",
        r"\s*Add to\s*",
        r"\s*Hide\s*",
        r"^\s*üîî\s*",
        r"\s*\d+\s*items?\s*$",
    ]

    for pattern in unwanted_patterns:
        cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE)

    # Usu≈Ñ nadmiarowe bia≈Çe znaki
    cleaned = re.sub(r"\s+", " ", cleaned).strip()

    # Sprawd≈∫ czy po czyszczeniu co≈õ zosta≈Ço
    if len(cleaned) < 2:
        return ""

    return cleaned


# --- Zaktualizowany blok testowy w __main__ ---
if __name__ == "__main__":
    print("Testowanie funkcji search_deku_deals i scrape_game_details...")

    # Test nowej funkcji parsowania kolekcji
    print("\n=== TESTOWANIE PARSOWANIA KOLEKCJI DEKUDEALS ===")
    collection_url = "https://www.dekudeals.com/collection/nbb76ddx3t"
    print(f"Testujƒô parsowanie kolekcji: {collection_url}")

    collection_result = scrape_dekudeals_collection(collection_url)
    if collection_result["success"]:
        print(f"‚úÖ Sukces! Znaleziono {collection_result['game_count']} gier:")
        for i, game in enumerate(collection_result["games"][:10], 1):
            print(f"  {i:2d}. {game}")
        if collection_result["game_count"] > 10:
            print(f"     ... i {collection_result['game_count'] - 10} wiƒôcej gier")
    else:
        print(f"‚ùå B≈ÇƒÖd: {collection_result['error']}")

    print("\n=== TESTOWANIE WYSZUKIWANIA POJEDYNCZYCH GIER ===")
    test_games = [
        "The Legend of Zelda: Tears of the Kingdom",
        "Hollow Knight",
        "Cuphead",
        "Celeste",  # Dodaj innƒÖ grƒô, ≈ºeby sprawdziƒá, czy dzia≈Ça dla r√≥≈ºnych
        "Super Mario Odyssey",
        "Witcher 3",
        "baba is you",
        "Transistor",
        "streets of rogue",
        "the last of us",
    ]

    for game_name in test_games:
        print(f"\n--- Przetwarzam grƒô: {game_name} ---")
        game_url = search_deku_deals(game_name)
        if game_url:
            game_details = scrape_game_details(game_url)
            if game_details:
                print(f"Pomy≈õlnie zesrapowano dane dla {game_name}.")
            else:
                print(f"Nie uda≈Ço siƒô zesrapowaƒá szczeg√≥≈Ç√≥w dla {game_name}.")
        else:
            print(f"Nie znaleziono URL dla {game_name}.")
        print("-" * 30)
